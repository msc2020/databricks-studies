## Generative AI Application Evaluation and Governance

### Course description

>This course introduces learners to evaluating and governing generative AI systems. First, you’ll explore the meaning behind and motivation for building evaluation and governance/security systems. Next, we’ll connect evaluation and governance systems to the Databricks Data Intelligence Platform. Third, we’ll teach you about a variety of evaluation techniques for specific components and types of applications. Finally, the course will conclude with an analysis of evaluating entire AI systems with respect to performance and cost.

- https://customer-academy.databricks.com/learn/course/2717/generative-ai-application-evaluation-and-governance?hash=e6c1cff98a4626e3ff8c7eb6cae75b7deaf232ce&generated_by=1410453

### Practical experiments

Through computational implementations, we explored security and observability aspects of LLMs using Databricks and MLflow. We compared the responses of simple assistants to suspicious/illicit prompts with those generated by an assistant equipped with a guardrail layer, implemented as an attempt to mitigate unexpected or unsafe outcomes. We also built a test-oriented benchmark to evaluate summarization assistants, assessing the performance of both models using MLflow to produce traceable outputs and to enable several Databricks platform features, such as monitoring multiple metrics, logging, and visualization through interactive dashboards.

Addicionally, we used LLMs as judges. For this purpose, we created an assistant specialized in psychological metrics and evaluated a simple Q&A data based on a hypothetical formulation of the "Big Seven" personality traits, which classifies personality traits from textual user inputs and interactions with a LLM judge.


### Notebooks:

  - [illict-prompts](./[illicit-prompts]generative-ai-application-evaluation-and-governance.ipynb)
  - [implementing-a-guardrail-layer](./[implementing-a-guardrail-layer]generative-ai-application-evaluation-and-governance.ipynb)
  - [benchmark-eval-lllms](./[benchmark-eval-lllms]generative-ai-application-evaluation-and-governance.ipynb)
  - [lllm-as-a-judge](./[benchmark-eval-lllms]generative-ai-application-evaluation-and-governance.ipynb)
